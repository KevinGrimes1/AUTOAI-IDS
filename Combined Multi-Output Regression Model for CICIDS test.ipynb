{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_dataset:  (85093, 8)\n",
      "        Destination Port   Total Length of Bwd Packets  \\\n",
      "0               0.006760                  8.908108e-06   \n",
      "1               0.000809                  1.963964e-07   \n",
      "2               0.001221                  2.300901e-06   \n",
      "3               0.000809                  6.270270e-07   \n",
      "4               0.007096                  0.000000e+00   \n",
      "...                  ...                           ...   \n",
      "85088           0.561091                  0.000000e+00   \n",
      "85089           0.001221                  1.392793e-05   \n",
      "85090           0.000809                  7.675676e-07   \n",
      "85091           0.001221                  2.089189e-05   \n",
      "85092           0.000809                  1.981982e-07   \n",
      "\n",
      "        Bwd Packet Length Mean   Packet Length Std   Packet Length Variance  \\\n",
      "0                     0.057846            0.293243                 0.085992   \n",
      "1                     0.028057            0.007193                 0.000052   \n",
      "2                     0.082177            0.107055                 0.011461   \n",
      "3                     0.044789            0.015881                 0.000252   \n",
      "4                     0.000000            0.003511                 0.000012   \n",
      "...                        ...                 ...                      ...   \n",
      "85088                 0.000000            0.000000                 0.000000   \n",
      "85089                 0.104723            0.109255                 0.011937   \n",
      "85090                 0.054827            0.015881                 0.000252   \n",
      "85091                 0.497436            0.327352                 0.107160   \n",
      "85092                 0.028315            0.007193                 0.000052   \n",
      "\n",
      "        Avg Bwd Segment Size   Subflow Bwd Bytes   Init_Win_bytes_backward  \n",
      "0                   0.057846        8.910296e-06                  0.008072  \n",
      "1                   0.028057        1.964446e-07                 -0.000015  \n",
      "2                   0.082177        2.301466e-06                  0.000488  \n",
      "3                   0.044789        6.271810e-07                 -0.000015  \n",
      "4                   0.000000        0.000000e+00                 -0.000015  \n",
      "...                      ...                 ...                       ...  \n",
      "85088               0.000000        0.000000e+00                  0.003662  \n",
      "85089               0.104723        1.393135e-05                  0.007813  \n",
      "85090               0.054827        7.677561e-07                 -0.000015  \n",
      "85091               0.497436        2.089702e-05                  0.003586  \n",
      "85092               0.028315        1.982469e-07                 -0.000015  \n",
      "\n",
      "[85093 rows x 8 columns]\n",
      "Shape of combined probabilities before na imputation drop:  (85093, 64)\n",
      "          ADA-0     ADA-1     ADA-2         ADA-3     ADA-4         ADA-5  \\\n",
      "0      0.155135  0.145887  0.151767  2.387689e-12  0.147827  1.940419e-13   \n",
      "1      0.155135  0.145887  0.151767  2.387689e-12  0.147827  1.940419e-13   \n",
      "2      0.155135  0.145887  0.151767  2.387689e-12  0.147827  1.940419e-13   \n",
      "3      0.155135  0.145887  0.151767  2.387689e-12  0.147827  1.940419e-13   \n",
      "4      0.164827  0.167249  0.136614  2.093817e-12  0.129414  2.636478e-13   \n",
      "...         ...       ...       ...           ...       ...           ...   \n",
      "85088  0.184512  0.183439  0.000324  1.925213e-11  0.000305  6.305017e-01   \n",
      "85089  0.155135  0.145887  0.151767  2.387689e-12  0.147827  1.940419e-13   \n",
      "85090  0.155135  0.145887  0.151767  2.387689e-12  0.147827  1.940419e-13   \n",
      "85091  0.163868  0.151765  0.166304  2.090805e-12  0.131379  1.721566e-13   \n",
      "85092  0.155135  0.145887  0.151767  2.387689e-12  0.147827  1.940419e-13   \n",
      "\n",
      "          ADA-6     ADA-7     ADA-8  KNN-0  ...     RF-17     SGD-0  \\\n",
      "0      0.106722  0.145691  0.146972    1.0  ...  1.000000  0.011439   \n",
      "1      0.106722  0.145691  0.146972    1.0  ...  0.999982  0.062568   \n",
      "2      0.106722  0.145691  0.146972    1.0  ...  0.999908  0.020874   \n",
      "3      0.106722  0.145691  0.146972    1.0  ...  0.999982  0.049587   \n",
      "4      0.148850  0.128710  0.124338    1.0  ...  0.997105  0.086997   \n",
      "...         ...       ...       ...    ...  ...       ...       ...   \n",
      "85088  0.000325  0.000303  0.000291    1.0  ...  0.990529  0.071326   \n",
      "85089  0.106722  0.145691  0.146972    1.0  ...  0.999982  0.015500   \n",
      "85090  0.106722  0.145691  0.146972    1.0  ...  0.999982  0.043977   \n",
      "85091  0.097137  0.154956  0.134591    1.0  ...  0.999895  0.000028   \n",
      "85092  0.106722  0.145691  0.146972    1.0  ...  0.999982  0.062382   \n",
      "\n",
      "              SGD-1     SGD-2     SGD-3     SGD-4     SGD-5     SGD-6  \\\n",
      "0      1.399803e-04  0.010037  0.043110  0.038281  0.042988  0.053789   \n",
      "1      3.620701e-02  0.061836  0.065050  0.063891  0.064930  0.058575   \n",
      "2      6.535273e-04  0.018761  0.050245  0.042411  0.049760  0.055629   \n",
      "3      1.461219e-02  0.047951  0.061623  0.057824  0.061376  0.057952   \n",
      "4      1.249802e-01  0.088673  0.070279  0.074475  0.070390  0.059479   \n",
      "...             ...       ...       ...       ...       ...       ...   \n",
      "85088  8.202459e-02  0.071889  0.064937  0.071391  0.065648  0.058909   \n",
      "85089  2.072079e-04  0.013531  0.047018  0.037177  0.046433  0.054916   \n",
      "85090  9.003515e-03  0.042022  0.059975  0.054734  0.059664  0.057645   \n",
      "85091  3.173674e-14  0.000014  0.009571  0.002379  0.009015  0.039700   \n",
      "85092  3.578173e-02  0.061635  0.065006  0.063803  0.064884  0.058567   \n",
      "\n",
      "          SGD-7     SGD-8  \n",
      "0      0.056376  0.055186  \n",
      "1      0.056696  0.057981  \n",
      "2      0.056849  0.056041  \n",
      "3      0.056787  0.057552  \n",
      "4      0.056526  0.058610  \n",
      "...         ...       ...  \n",
      "85088  0.056526  0.058012  \n",
      "85089  0.056912  0.055522  \n",
      "85090  0.056842  0.057333  \n",
      "85091  0.052136  0.043648  \n",
      "85092  0.056697  0.057976  \n",
      "\n",
      "[85093 rows x 64 columns]\n",
      "shape of Combined probabilites after imputation:  (85093, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2722: RuntimeWarning: invalid value encountered in subtract\n",
      "  np.interp(X_col_finite, quantiles, self.references_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ADA-0     ADA-1     ADA-2     ADA-3     ADA-4     ADA-5     ADA-6  \\\n",
      "0      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "1      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "2      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "3      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "4      0.775275  0.776276  0.231231  0.099099  0.236236  0.768769  1.000000   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "85088  0.892392  1.000000  0.117117  0.887387  0.119119  0.895395  0.115115   \n",
      "85089  0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "85090  0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "85091  0.722222  0.719219  1.000000  0.000000  0.292292  0.000000  0.257257   \n",
      "85092  0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "\n",
      "          ADA-7     ADA-8  KNN-0  ...     RF-17     SGD-0     SGD-1     SGD-2  \\\n",
      "0      0.507508  1.000000    1.0  ...  1.000000  0.166849  0.169521  0.167126   \n",
      "1      0.507508  1.000000    1.0  ...  0.728729  0.469025  0.414414  0.444822   \n",
      "2      0.507508  1.000000    1.0  ...  0.429429  0.195299  0.192787  0.193909   \n",
      "3      0.507508  1.000000    1.0  ...  0.728729  0.285285  0.281281  0.284284   \n",
      "4      0.233734  0.232733    1.0  ...  0.367868  0.865767  0.860944  0.866817   \n",
      "...         ...       ...    ...  ...       ...       ...       ...       ...   \n",
      "85088  0.119119  0.116617    1.0  ...  0.299800  0.668694  0.751351  0.667733   \n",
      "85089  0.507508  1.000000    1.0  ...  0.728729  0.180745  0.174943  0.180222   \n",
      "85090  0.507508  1.000000    1.0  ...  0.728729  0.255033  0.250859  0.252224   \n",
      "85091  1.000000  0.288288    1.0  ...  0.412412  0.059262  0.059754  0.061336   \n",
      "85092  0.507508  1.000000    1.0  ...  0.728729  0.464306  0.411173  0.439443   \n",
      "\n",
      "          SGD-3     SGD-4     SGD-5     SGD-6     SGD-7     SGD-8  \n",
      "0      0.160652  0.176155  0.163028  0.160865  0.115310  0.166046  \n",
      "1      0.535570  0.404092  0.526850  0.454027  0.738477  0.531532  \n",
      "2      0.189835  0.188152  0.191111  0.194097  0.922627  0.188553  \n",
      "3      0.316041  0.275938  0.284284  0.287287  0.872406  0.284562  \n",
      "4      0.845515  0.852942  0.861422  0.861357  0.145646  0.857014  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "85088  0.526763  0.745307  0.583458  0.653755  0.271772  0.546949  \n",
      "85089  0.175759  0.173774  0.177177  0.179864  0.956876  0.173737  \n",
      "85090  0.252048  0.243632  0.252737  0.258826  0.917919  0.248335  \n",
      "85091  0.060713  0.063471  0.058379  0.062512  0.061566  0.060174  \n",
      "85092  0.532551  0.401408  0.524540  0.447522  0.741172  0.528644  \n",
      "\n",
      "[85093 rows x 64 columns]\n",
      "shape of Combined probabilites after imputation and scaling:  (85093, 64)\n",
      "          ADA-0     ADA-1     ADA-2     ADA-3     ADA-4     ADA-5     ADA-6  \\\n",
      "0      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "1      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "2      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "3      0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "4      0.775275  0.776276  0.231231  0.099099  0.236236  0.768769  1.000000   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "85088  0.892392  1.000000  0.117117  0.887387  0.119119  0.895395  0.115115   \n",
      "85089  0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "85090  0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "85091  0.722222  0.719219  1.000000  0.000000  0.292292  0.000000  0.257257   \n",
      "85092  0.420921  0.269770  0.656657  0.524024  0.593594  0.494995  0.562563   \n",
      "\n",
      "          ADA-7     ADA-8  KNN-0  ...     RF-17     SGD-0     SGD-1     SGD-2  \\\n",
      "0      0.507508  1.000000    1.0  ...  1.000000  0.166849  0.169521  0.167126   \n",
      "1      0.507508  1.000000    1.0  ...  0.728729  0.469025  0.414414  0.444822   \n",
      "2      0.507508  1.000000    1.0  ...  0.429429  0.195299  0.192787  0.193909   \n",
      "3      0.507508  1.000000    1.0  ...  0.728729  0.285285  0.281281  0.284284   \n",
      "4      0.233734  0.232733    1.0  ...  0.367868  0.865767  0.860944  0.866817   \n",
      "...         ...       ...    ...  ...       ...       ...       ...       ...   \n",
      "85088  0.119119  0.116617    1.0  ...  0.299800  0.668694  0.751351  0.667733   \n",
      "85089  0.507508  1.000000    1.0  ...  0.728729  0.180745  0.174943  0.180222   \n",
      "85090  0.507508  1.000000    1.0  ...  0.728729  0.255033  0.250859  0.252224   \n",
      "85091  1.000000  0.288288    1.0  ...  0.412412  0.059262  0.059754  0.061336   \n",
      "85092  0.507508  1.000000    1.0  ...  0.728729  0.464306  0.411173  0.439443   \n",
      "\n",
      "          SGD-3     SGD-4     SGD-5     SGD-6     SGD-7     SGD-8  \n",
      "0      0.160652  0.176155  0.163028  0.160865  0.115310  0.166046  \n",
      "1      0.535570  0.404092  0.526850  0.454027  0.738477  0.531532  \n",
      "2      0.189835  0.188152  0.191111  0.194097  0.922627  0.188553  \n",
      "3      0.316041  0.275938  0.284284  0.287287  0.872406  0.284562  \n",
      "4      0.845515  0.852942  0.861422  0.861357  0.145646  0.857014  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "85088  0.526763  0.745307  0.583458  0.653755  0.271772  0.546949  \n",
      "85089  0.175759  0.173774  0.177177  0.179864  0.956876  0.173737  \n",
      "85090  0.252048  0.243632  0.252737  0.258826  0.917919  0.248335  \n",
      "85091  0.060713  0.063471  0.058379  0.062512  0.061566  0.060174  \n",
      "85092  0.532551  0.401408  0.524540  0.447522  0.741172  0.528644  \n",
      "\n",
      "[85093 rows x 64 columns]\n",
      "Shape of combined probabilities after imputation, scaling, and matching test:  (85093, 64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 209\u001b[0m\n\u001b[0;32m    207\u001b[0m multioutput_regressor_RF \u001b[38;5;241m=\u001b[39m MultiOutputRegressor(regressor)\n\u001b[0;32m    208\u001b[0m start_train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 209\u001b[0m \u001b[43mmultioutput_regressor_RF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scale_standard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m end_train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    211\u001b[0m training_time \u001b[38;5;241m=\u001b[39m end_train_time \u001b[38;5;241m-\u001b[39m start_train_time\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\multioutput.py:242\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe base estimator should implement a fit method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_validation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    245\u001b[0m     check_classification_targets(y)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:607\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    605\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m--> 607\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_separately:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;66;03m# We need this because some estimators validate X and y\u001b[39;00m\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;66;03m# separately, and in general, separately calling check_array()\u001b[39;00m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;66;03m# on X and y isn't equivalent to just calling check_X_y()\u001b[39;00m\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;66;03m# :(\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1172\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[1;32m-> 1172\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1182\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "#Model using RandomForestRegressor, ADA, SVM (SVR), LGBM.\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "################################################################################\n",
    "##########################FUNCTION DEFINITIONS##################################\n",
    "def frequency_in_top_k(df, k):\n",
    "    frequency_dict = {column: 0 for column in df.columns}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #sort the row to get the top k model names\n",
    "        top_k_models = row.sort_values(ascending=False).head(k).index\n",
    "\n",
    "        #update the frequency count for each model in the top k\n",
    "        for model in top_k_models:\n",
    "            if model in frequency_dict:\n",
    "                frequency_dict[model] +=1\n",
    "\n",
    "    return frequency_dict\n",
    "\n",
    "################################################################################\n",
    "#############################DATA PREPROCESSING#################################\n",
    "\n",
    "column_names = [\n",
    "    'Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
    "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
    "    'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s',\n",
    "    'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
    "    'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean',\n",
    "    'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
    "    'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
    "    'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
    "    'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',\n",
    "    'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
    "    'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size', 'Fwd Header Length',\n",
    "    'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk',\n",
    "    'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
    "    'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
    "    'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min',\n",
    "    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label'\n",
    "]\n",
    "\n",
    "\n",
    "prob_ada_column_names = [\n",
    "    'ADA-0', 'ADA-1', 'ADA-2', 'ADA-3', 'ADA-4', 'ADA-5', 'ADA-6', 'ADA-7', 'ADA-8'\n",
    "]\n",
    "\n",
    "prob_knn_column_names = [\n",
    "    'KNN-0', 'KNN-1', 'KNN-2', 'KNN-3', 'KNN-4', 'KNN-5'\n",
    "]\n",
    "\n",
    "prob_lgbm_column_names = [\n",
    "    'LGBM-0', 'LGBM-1', 'LGBM-2', 'LGBM-3'\n",
    "]\n",
    "\n",
    "prob_dnn_column_names = [\n",
    "    'DNN-0', 'DNN-1', 'DNN-2', 'DNN-3', 'DNN-4', 'DNN-5', 'DNN-6', 'DNN-7', 'DNN-8'\n",
    "]\n",
    "\n",
    "prob_mlp_column_names = [\n",
    "    'MLP-0', 'MLP-1', 'MLP-2', 'MLP-3', 'MLP-4', 'MLP-5', 'MLP-6', 'MLP-7', 'MLP-8'\n",
    "]\n",
    "\n",
    "prob_rf_column_names = [\n",
    "    'RF-0', 'RF-1', 'RF-2', 'RF-3', 'RF-4', 'RF-5', 'RF-6', 'RF-7', \n",
    "    'RF-8', 'RF-9', 'RF-10', 'RF-11', 'RF-12', 'RF-13', 'RF-14', 'RF-15',\n",
    "    'RF-16', 'RF-17'\n",
    "]\n",
    "\n",
    "prob_sgd_column_names = [\n",
    "    'SGD-0', 'SGD-1', 'SGD-2', 'SGD-3', 'SGD-4', 'SGD-5', 'SGD-6', 'SGD-7', 'SGD-8'\n",
    "]\n",
    "\n",
    "prob_output_column_names = [\n",
    "    'ADA-0', 'ADA-1', 'ADA-2', 'ADA-3', 'ADA-4', 'ADA-5', 'ADA-6', 'ADA-7', 'ADA-8',\n",
    "    'KNN-0', 'KNN-1', 'KNN-2', 'KNN-3', 'KNN-4', 'KNN-5',\n",
    "    'LGBM-0', 'LGBM-1', 'LGBM-2', 'LGBM-3',\n",
    "    'DNN-0', 'DNN-1', 'DNN-2', 'DNN-3', 'DNN-4', 'DNN-5', 'DNN-6', 'DNN-7', 'DNN-8',\n",
    "    'MLP-0', 'MLP-1', 'MLP-2', 'MLP-3', 'MLP-4', 'MLP-5', 'MLP-6', 'MLP-7', 'MLP-8',\n",
    "    'RF-0', 'RF-1', 'RF-2', 'RF-3', 'RF-4', 'RF-5', 'RF-6', 'RF-7', \n",
    "    'RF-8', 'RF-9', 'RF-10', 'RF-11', 'RF-12', 'RF-13', 'RF-14', 'RF-15',\n",
    "    'RF-16', 'RF-17',\n",
    "    'SGD-0', 'SGD-1', 'SGD-2', 'SGD-3', 'SGD-4', 'SGD-5', 'SGD-6', 'SGD-7', 'SGD-8'\n",
    "]\n",
    "\n",
    "#Beginning of Test Data Setup\n",
    "test_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\cicids_test.csv\"\n",
    "test_dataset = pd.read_csv(test_path)\n",
    "label_and_istrain = test_dataset.columns[:-2]\n",
    "test_dataset = pd.read_csv(test_path, usecols=label_and_istrain)\n",
    "print(\"Shape of test_dataset: \", test_dataset.shape)\n",
    "#samples_test = test_dataset.drop('Label', axis=1)\n",
    "#dropping final column with label\n",
    "#test_dataset.drop(test_dataset.columns[-1], axis=1, inplace=True)\n",
    "#dropping rows with na values\n",
    "#test_dataset.dropna(axis=0, inplace=True)\n",
    "#dropping columns with na values\n",
    "#test_dataset.dropna(axis=1, inplace=True)\n",
    "#removing infinity values\n",
    "#test_dataset[np.isinf(test_dataset)] = np.finfo(np.float32).max\n",
    "print(test_dataset)\n",
    "#print(\"Shape of test_dataset: \", test_dataset.shape)\n",
    "\n",
    "#Beginning of probability usage\n",
    "\n",
    "knn_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\knn_selected_probabilities_new.csv\"\n",
    "knn_probabilities = pd.read_csv(knn_prob_path, header=None, names=prob_knn_column_names, dtype='object')\n",
    "knn_probabilities = knn_probabilities.iloc[1:]#removing label\n",
    "knn_probabilities = knn_probabilities.reset_index(drop=True)\n",
    "\n",
    "prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\ada_cicids_probabilities.csv\"\n",
    "ada_probabilities = pd.read_csv(prob_path, header=None, names=prob_ada_column_names, dtype='object')\n",
    "ada_probabilities = ada_probabilities.iloc[1:] #removing label\n",
    "ada_probabilities = ada_probabilities.head(len(knn_probabilities))\n",
    "ada_probabilities = ada_probabilities.reset_index(drop=True)\n",
    "\n",
    "\n",
    "lgbm_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\lgbm_hyperparameter_probabilities.csv\"\n",
    "lgbm_probabilities = pd.read_csv(lgbm_prob_path, header=None, names=prob_lgbm_column_names, dtype='object')\n",
    "lgbm_probabilities = lgbm_probabilities.iloc[1:]#removing label\n",
    "lgbm_probabilities = lgbm_probabilities.reset_index(drop=True)\n",
    "\n",
    "\n",
    "dnn_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\dnn_selected_probabilities_cicidsv1.csv\"\n",
    "dnn_probabilities = pd.read_csv(dnn_prob_path, header=None, names=prob_dnn_column_names, dtype='object')\n",
    "dnn_probabilities = dnn_probabilities.iloc[1:]#removing label\n",
    "dnn_probabilities = dnn_probabilities.reset_index(drop=True)\n",
    "\n",
    "mlp_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\mlp_selected_probabilities.csv\"\n",
    "mlp_probabilities = pd.read_csv(mlp_prob_path, header=None, names=prob_mlp_column_names, dtype='object')\n",
    "mlp_probabilities = mlp_probabilities.iloc[1:]#removing label\n",
    "mlp_probabilities = mlp_probabilities.reset_index(drop=True)\n",
    "\n",
    "rf_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\rf_selected_probabilities.csv\"\n",
    "rf_probabilities = pd.read_csv(rf_prob_path, header=None, names=prob_rf_column_names, dtype='object')\n",
    "rf_probabilities = rf_probabilities.iloc[1:]#removing label\n",
    "rf_probabilities = rf_probabilities.reset_index(drop=True)\n",
    "\n",
    "sgd_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\sgd_selected_probabilities.csv\"\n",
    "sgd_probabilities = pd.read_csv(sgd_prob_path, header=None, names=prob_sgd_column_names, dtype='object')\n",
    "sgd_probabilities = sgd_probabilities.iloc[1:]#removing label\n",
    "sgd_probabilities = sgd_probabilities.reset_index(drop=True)\n",
    "\n",
    "combined_probabilities = pd.concat([ada_probabilities, knn_probabilities, lgbm_probabilities, dnn_probabilities, mlp_probabilities,\n",
    "                                    rf_probabilities, sgd_probabilities],axis = 1) \n",
    "print(\"Shape of combined probabilities before na imputation drop: \", combined_probabilities.shape)\n",
    "#Imputation necessary to take care of large amount of NaN values within dataset\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "mean_imputed_probabilities = pd.DataFrame(mean_imputer.fit_transform(combined_probabilities), columns=prob_output_column_names)\n",
    "print(mean_imputed_probabilities)\n",
    "print(\"shape of Combined probabilites after imputation: \", mean_imputed_probabilities.shape)\n",
    "#removing infinity values\n",
    "mean_imputed_probabilities[np.isinf(mean_imputed_probabilities)] = np.finfo(np.float32).max\n",
    "#combi#dropping rows with na values\n",
    "#combined_probabilities.dropna(axis=0, inplace=True)\n",
    "#dropping columns with na values\n",
    "#combined_probabilities.dropna(axis=1, inplace=True)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "#combined_probabilities = scaler.fit_transform(mean_imputed_probabilities)\n",
    "combined_probabilities = scaler.fit_transform(mean_imputed_probabilities)\n",
    "# Save the combined data to a new CSV file\n",
    "combined_probabilities = pd.DataFrame(combined_probabilities, columns = prob_output_column_names)\n",
    "print(combined_probabilities)\n",
    "print(\"shape of Combined probabilites after imputation and scaling: \", combined_probabilities.shape)\n",
    "#trimming first dataset to match probabilities\n",
    "#combined_probabilities = combined_probabilities.sample(n=22544, random_state=42)\n",
    "#test_dataset = test_dataset[:len(combined_probabilities)]\n",
    "#combined_probabilities = combined_probabilities[:len(test_dataset)]\n",
    "combined_probabilities.to_csv('combined_probabilities.csv', index=False)\n",
    "print(combined_probabilities)\n",
    "print(\"Shape of combined probabilities after imputation, scaling, and matching test: \", combined_probabilities.shape)\n",
    "################################################################################\n",
    "#############################TRAINING###########################################\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(test_dataset, combined_probabilities, test_size=.2, random_state=42)\n",
    "Y_train_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Y_train_CICIDS.csv\"\n",
    "Y_train = pd.DataFrame(Y_train)\n",
    "Y_train.to_csv(Y_train_prob_path, index=False)\n",
    "Y_test_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Y_test_CICIDS.csv\"\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "Y_test.to_csv(Y_test_prob_path, index=False)\n",
    "\n",
    "#RFTRAIN\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale= scaler.fit_transform(X_test)\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scale_standard = standard_scaler.fit_transform(X_train_scale)\n",
    "X_test_scale_standard = standard_scaler.fit_transform(X_test_scale)\n",
    "regressor = RandomForestRegressor(random_state=42)\n",
    "multioutput_regressor_RF = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "multioutput_regressor_RF.fit(X_train_scale_standard, Y_train)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"RF Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "#ADATRAIN\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale= scaler.fit_transform(X_test)\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scale_standard = standard_scaler.fit_transform(X_train_scale)\n",
    "X_test_scale_standard = standard_scaler.fit_transform(X_test_scale)\n",
    "regressor = AdaBoostRegressor(random_state=42)\n",
    "multioutput_regressor_ADA = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "multioutput_regressor_ADA.fit(X_train_scale_standard, Y_train)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"AdaBoostRegressor Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "#SVMTRAIN\n",
    "scaler = MinMaxScaler()\n",
    "X_train_SVM = X_train[:5000]\n",
    "X_test_SVM = X_train[:5000]\n",
    "X_train_scale_SVM = scaler.fit_transform(X_train_SVM)\n",
    "X_test_scale_SVM= scaler.fit_transform(X_test_SVM)\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scale_standard = standard_scaler.fit_transform(X_train_scale_SVM)\n",
    "X_test_scale_standard = standard_scaler.fit_transform(X_test_scale_SVM)\n",
    "regressor = SVR(kernel='rbf')\n",
    "multioutput_regressor_SVM = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "Y_train_SVM = Y_train[:5000]\n",
    "multioutput_regressor_SVM.fit(X_train_scale_standard, Y_train_SVM)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"SVR Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "#LGBMTRAIN\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train)\n",
    "X_test_scale= scaler.fit_transform(X_test)\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_scale_standard = standard_scaler.fit_transform(X_train_scale)\n",
    "X_test_scale_standard = standard_scaler.fit_transform(X_test_scale)\n",
    "regressor = LGBMRegressor(random_state=42)\n",
    "multioutput_regressor_LGBM = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "multioutput_regressor_LGBM.fit(X_train_scale_standard, Y_train)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"LGBMRegressor Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "###################################################################################\n",
    "#############################RF TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_RF.predict(X_test_scale_standard)\n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"RF Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_RF.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns = prob_output_column_names)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns = prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_RF.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_1_RF.csv\"\n",
    "topk_1_RF = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_RF])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_5_RF.csv\"\n",
    "topk_5_RF = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_RF])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_10_RF.csv\"\n",
    "topk_10_RF = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_RF])\n",
    "df_topk_10.to_csv(topk_10, index=False)\n",
    "####################################################################################\n",
    "#############################ADA TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_ADA.predict(X_test_scale_standard)\n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"ADABoostRegressor Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_ADA.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns = prob_output_column_names)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns = prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_ADA.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_1_ADA.csv\"\n",
    "topk_1_RF = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_RF])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_5_ADA.csv\"\n",
    "topk_5_RF = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_RF])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_10_ADA.csv\"\n",
    "topk_10_RF = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_RF])\n",
    "df_topk_10.to_csv(topk_10, index=False)\n",
    "####################################################################################\n",
    "#############################SVM TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_SVM.predict(X_test_scale_standard)\n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"SVM Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_SVM.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns = prob_output_column_names)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns = prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_SVM.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_1_SVM.csv\"\n",
    "topk_1_RF = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_RF])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_5_SVM.csv\"\n",
    "topk_5_RF = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_RF])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_10_SVM.csv\"\n",
    "topk_10_RF = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_RF])\n",
    "df_topk_10.to_csv(topk_10, index=False)\n",
    "####################################################################################\n",
    "#############################LGBM TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_LGBM.predict(X_test_scale_standard)\n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"LGBM Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_LGBM.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns=prob_output_column_names)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns=prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_probability_output_LGBM.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_1_LGBM.csv\"\n",
    "topk_1_RF = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_RF])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_5_LGBM.csv\"\n",
    "topk_5_RF = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_RF])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH CICIDS\\Results\\CICIDS_topk_10_LGBM.csv\"\n",
    "topk_10_RF = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_RF])\n",
    "df_topk_10.to_csv(topk_10, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
