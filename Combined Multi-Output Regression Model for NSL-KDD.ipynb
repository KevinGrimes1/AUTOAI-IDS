{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples before encoding        duration protocol_type   service  flag  src_bytes  dst_bytes  land  \\\n",
      "0             0           tcp   private   REJ          0          0     0   \n",
      "1             0           tcp   private   REJ          0          0     0   \n",
      "2             2           tcp  ftp_data    SF      12983          0     0   \n",
      "3             0          icmp     eco_i    SF         20          0     0   \n",
      "4             1           tcp    telnet  RSTO          0         15     0   \n",
      "...         ...           ...       ...   ...        ...        ...   ...   \n",
      "22539         0           tcp      smtp    SF        794        333     0   \n",
      "22540         0           tcp      http    SF        317        938     0   \n",
      "22541         0           tcp      http    SF      54540       8314     0   \n",
      "22542         0           udp  domain_u    SF         42         42     0   \n",
      "22543         0           tcp    sunrpc   REJ          0          0     0   \n",
      "\n",
      "       wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0                   0       0    0  ...                  10   \n",
      "1                   0       0    0  ...                   1   \n",
      "2                   0       0    0  ...                  86   \n",
      "3                   0       0    0  ...                  57   \n",
      "4                   0       0    0  ...                  86   \n",
      "...               ...     ...  ...  ...                 ...   \n",
      "22539               0       0    0  ...                 141   \n",
      "22540               0       0    0  ...                 255   \n",
      "22541               0       0    2  ...                 255   \n",
      "22542               0       0    0  ...                 252   \n",
      "22543               0       0    0  ...                  21   \n",
      "\n",
      "       dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                        0.04                    0.06   \n",
      "1                        0.00                    0.06   \n",
      "2                        0.61                    0.04   \n",
      "3                        1.00                    0.00   \n",
      "4                        0.31                    0.17   \n",
      "...                       ...                     ...   \n",
      "22539                    0.72                    0.06   \n",
      "22540                    1.00                    0.00   \n",
      "22541                    1.00                    0.00   \n",
      "22542                    0.99                    0.01   \n",
      "22543                    0.08                    0.03   \n",
      "\n",
      "       dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                             0.00                         0.00   \n",
      "1                             0.00                         0.00   \n",
      "2                             0.61                         0.02   \n",
      "3                             1.00                         0.28   \n",
      "4                             0.03                         0.02   \n",
      "...                            ...                          ...   \n",
      "22539                         0.01                         0.01   \n",
      "22540                         0.01                         0.01   \n",
      "22541                         0.00                         0.00   \n",
      "22542                         0.00                         0.00   \n",
      "22543                         0.00                         0.00   \n",
      "\n",
      "       dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                      0.00                       0.0                  1.00   \n",
      "1                      0.00                       0.0                  1.00   \n",
      "2                      0.00                       0.0                  0.00   \n",
      "3                      0.00                       0.0                  0.00   \n",
      "4                      0.00                       0.0                  0.83   \n",
      "...                     ...                       ...                   ...   \n",
      "22539                  0.01                       0.0                  0.00   \n",
      "22540                  0.01                       0.0                  0.00   \n",
      "22541                  0.00                       0.0                  0.07   \n",
      "22542                  0.00                       0.0                  0.00   \n",
      "22543                  0.00                       0.0                  0.44   \n",
      "\n",
      "       dst_host_srv_rerror_rate  severity_score  \n",
      "0                          1.00              21  \n",
      "1                          1.00              21  \n",
      "2                          0.00              21  \n",
      "3                          0.00              15  \n",
      "4                          0.71              11  \n",
      "...                         ...             ...  \n",
      "22539                      0.00              21  \n",
      "22540                      0.00              21  \n",
      "22541                      0.07              15  \n",
      "22542                      0.00              21  \n",
      "22543                      1.00              14  \n",
      "\n",
      "[22544 rows x 42 columns]\n",
      "samples after encoding        duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
      "0             0          0          0     0               0       0    0   \n",
      "1             0          0          0     0               0       0    0   \n",
      "2             2      12983          0     0               0       0    0   \n",
      "3             0         20          0     0               0       0    0   \n",
      "4             1          0         15     0               0       0    0   \n",
      "...         ...        ...        ...   ...             ...     ...  ...   \n",
      "22539         0        794        333     0               0       0    0   \n",
      "22540         0        317        938     0               0       0    0   \n",
      "22541         0      54540       8314     0               0       0    2   \n",
      "22542         0         42         42     0               0       0    0   \n",
      "22543         0          0          0     0               0       0    0   \n",
      "\n",
      "       num_failed_logins  logged_in  num_compromised  ...  flag_REJ  \\\n",
      "0                      0          0                0  ...       1.0   \n",
      "1                      0          0                0  ...       1.0   \n",
      "2                      0          0                0  ...       0.0   \n",
      "3                      0          0                0  ...       0.0   \n",
      "4                      0          0                0  ...       0.0   \n",
      "...                  ...        ...              ...  ...       ...   \n",
      "22539                  0          1                0  ...       0.0   \n",
      "22540                  0          1                0  ...       0.0   \n",
      "22541                  0          1                1  ...       0.0   \n",
      "22542                  0          0                0  ...       0.0   \n",
      "22543                  0          0                0  ...       1.0   \n",
      "\n",
      "       flag_RSTO  flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  \\\n",
      "0            0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "1            0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "2            0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "3            0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "4            1.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "...          ...          ...        ...      ...      ...      ...      ...   \n",
      "22539        0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "22540        0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "22541        0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "22542        0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "22543        0.0          0.0        0.0      0.0      0.0      0.0      0.0   \n",
      "\n",
      "       flag_SF  flag_SH  \n",
      "0          0.0      0.0  \n",
      "1          0.0      0.0  \n",
      "2          1.0      0.0  \n",
      "3          1.0      0.0  \n",
      "4          0.0      0.0  \n",
      "...        ...      ...  \n",
      "22539      1.0      0.0  \n",
      "22540      1.0      0.0  \n",
      "22541      1.0      0.0  \n",
      "22542      1.0      0.0  \n",
      "22543      0.0      0.0  \n",
      "\n",
      "[22544 rows x 117 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined probabilites:  (22544, 64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 166\u001b[0m\n\u001b[0;32m    164\u001b[0m multioutput_regressor_RF \u001b[38;5;241m=\u001b[39m MultiOutputRegressor(regressor)\n\u001b[0;32m    165\u001b[0m start_train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 166\u001b[0m \u001b[43mmultioutput_regressor_RF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m end_train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    168\u001b[0m training_time \u001b[38;5;241m=\u001b[39m end_train_time \u001b[38;5;241m-\u001b[39m start_train_time\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\multioutput.py:273\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\multioutput.py:60\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m     58\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Model using RandomForestRegressor, ADA, SVM (SVR), LGBM.\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "################################################################################\n",
    "##########################FUNCTION DEFINITIONS##################################\n",
    "def frequency_in_top_k(df, k):\n",
    "    frequency_dict = {column: 0 for column in df.columns}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #sort the row to get the top k model names\n",
    "        top_k_models = row.sort_values(ascending=False).head(k).index\n",
    "\n",
    "        #update the frequency count for each model in the top k\n",
    "        for model in top_k_models:\n",
    "            if model in frequency_dict:\n",
    "                frequency_dict[model] +=1\n",
    "\n",
    "    return frequency_dict\n",
    "\n",
    "################################################################################\n",
    "#############################DATA PREPROCESSING#################################\n",
    "\n",
    "\n",
    "column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_error_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate', 'label', 'severity_score'\n",
    "]\n",
    "\n",
    "prob_ada_column_names = [\n",
    "    'ADA-0', 'ADA-1', 'ADA-2', 'ADA-3', 'ADA-4', 'ADA-5', 'ADA-6', 'ADA-7', 'ADA-8'\n",
    "]\n",
    "\n",
    "prob_knn_column_names = [\n",
    "    'KNN-0', 'KNN-1', 'KNN-2', 'KNN-3', 'KNN-4', 'KNN-5'\n",
    "]\n",
    "\n",
    "prob_lgbm_column_names = [\n",
    "    'LGBM-0', 'LGBM-1', 'LGBM-2', 'LGBM-3'\n",
    "]\n",
    "\n",
    "prob_dnn_column_names = [\n",
    "    'DNN-0', 'DNN-1', 'DNN-2', 'DNN-3', 'DNN-4', 'DNN-5', 'DNN-6', 'DNN-7', 'DNN-8'\n",
    "]\n",
    "\n",
    "prob_mlp_column_names = [\n",
    "    'MLP-0', 'MLP-1', 'MLP-2', 'MLP-3', 'MLP-4', 'MLP-5', 'MLP-6', 'MLP-7', 'MLP-8'\n",
    "]\n",
    "\n",
    "prob_rf_column_names = [\n",
    "    'RF-0', 'RF-1', 'RF-2', 'RF-3', 'RF-4', 'RF-5', 'RF-6', 'RF-7', \n",
    "    'RF-8', 'RF-9', 'RF-10', 'RF-11', 'RF-12', 'RF-13', 'RF-14', 'RF-15',\n",
    "    'RF-16', 'RF-17'\n",
    "]\n",
    "\n",
    "prob_sgd_column_names = [\n",
    "    'SGD-0', 'SGD-1', 'SGD-2', 'SGD-3', 'SGD-4', 'SGD-5', 'SGD-6', 'SGD-7', 'SGD-8'\n",
    "]\n",
    "\n",
    "prob_output_column_names = [\n",
    "    'ADA-0', 'ADA-1', 'ADA-2', 'ADA-3', 'ADA-4', 'ADA-5', 'ADA-6', 'ADA-7', 'ADA-8',\n",
    "    'KNN-0', 'KNN-1', 'KNN-2', 'KNN-3', 'KNN-4', 'KNN-5',\n",
    "    'LGBM-0', 'LGBM-1', 'LGBM-2', 'LGBM-3',\n",
    "    'DNN-0', 'DNN-1', 'DNN-2', 'DNN-3', 'DNN-4', 'DNN-5', 'DNN-6', 'DNN-7', 'DNN-8',\n",
    "    'MLP-0', 'MLP-1', 'MLP-2', 'MLP-3', 'MLP-4', 'MLP-5', 'MLP-6', 'MLP-7', 'MLP-8',\n",
    "    'RF-0', 'RF-1', 'RF-2', 'RF-3', 'RF-4', 'RF-5', 'RF-6', 'RF-7', \n",
    "    'RF-8', 'RF-9', 'RF-10', 'RF-11', 'RF-12', 'RF-13', 'RF-14', 'RF-15',\n",
    "    'RF-16', 'RF-17',\n",
    "    'SGD-0', 'SGD-1', 'SGD-2', 'SGD-3', 'SGD-4', 'SGD-5', 'SGD-6', 'SGD-7', 'SGD-8'\n",
    "]\n",
    "\n",
    "categorical_columns = [\"protocol_type\", \"service\", \"flag\"]\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "#Beginning of Test Data Setup\n",
    "test_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\KDDTest+.txt\"\n",
    "test_dataset = pd.read_csv(test_path, header=None, names=column_names)\n",
    "samples_test = test_dataset.drop('label', axis=1)\n",
    "\n",
    "print(\"samples before encoding\", samples_test)\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "# Fit and transform the categorical columns\n",
    "encoded_columns = pd.DataFrame(encoder.fit_transform(samples_test[categorical_columns]))\n",
    "\n",
    "# Get the names of the encoded columns\n",
    "encoded_columns.columns = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Concatenate the original DataFrame with the encoded columns\n",
    "data_encoded = pd.concat([samples_test.drop(categorical_columns, axis=1), encoded_columns], axis=1)\n",
    "\n",
    "print(\"samples after encoding\", data_encoded)\n",
    "\n",
    "#Beginning of probability usage\n",
    "prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\selected_probabilities_ada.csv\"\n",
    "ada_probabilities = pd.read_csv(prob_path, header=None, names=prob_ada_column_names)\n",
    "ada_probabilities = ada_probabilities.loc[1:] #removing label\n",
    "\n",
    "knn_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\knn_selected_probabilities.csv\"\n",
    "knn_probabilities = pd.read_csv(knn_prob_path, header=None, names=prob_knn_column_names)\n",
    "knn_probabilities = knn_probabilities.loc[1:]#removing label\n",
    "\n",
    "lgbm_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-kDD\\lgbm_hyperparameter_probabilities.csv\"\n",
    "lgbm_probabilities = pd.read_csv(lgbm_prob_path, header=None, names=prob_lgbm_column_names)\n",
    "lgbm_probabilities = lgbm_probabilities.loc[1:]#removing label\n",
    "lgbm_probabilities = lgbm_probabilities.head(len(knn_probabilities))\n",
    "\n",
    "dnn_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\dnn_selected_probabilities.csv\"\n",
    "dnn_probabilities = pd.read_csv(dnn_prob_path, header=None, names=prob_dnn_column_names)\n",
    "dnn_probabilities = dnn_probabilities.loc[1:]#removing label\n",
    "\n",
    "mlp_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\\\ECE RESEARCH NSL-KDD\\mlp_selected_probabilities.csv\"\n",
    "mlp_probabilities = pd.read_csv(mlp_prob_path, header=None, names=prob_mlp_column_names)\n",
    "mlp_probabilities = mlp_probabilities.loc[1:]#removing label\n",
    "\n",
    "rf_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\rf_selected_probabilities.csv\"\n",
    "rf_probabilities = pd.read_csv(rf_prob_path, header=None, names=prob_rf_column_names)\n",
    "rf_probabilities = rf_probabilities.loc[1:]#removing label\n",
    "\n",
    "sgd_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\sgd_selected_probabilities.csv\"\n",
    "sgd_probabilities = pd.read_csv(sgd_prob_path, header=None, names=prob_sgd_column_names)\n",
    "sgd_probabilities = sgd_probabilities.loc[1:]#removing label\n",
    "\n",
    "combined_probabilities = pd.concat([ada_probabilities, knn_probabilities, lgbm_probabilities, dnn_probabilities,\n",
    "                                     mlp_probabilities, rf_probabilities, sgd_probabilities],axis = 1) \n",
    "# Save the combined data to a new CSV file\n",
    "combined_probabilities.to_csv('combined_probabilities.csv', index=False)\n",
    "print(\"Combined probabilites: \", combined_probabilities.shape)\n",
    "\n",
    "################################################################################\n",
    "#############################TRAINING###########################################\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_encoded, combined_probabilities, test_size=.2, random_state=42)\n",
    "#RFTRAIN\n",
    "Y_train_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\Y_train_NSL-KDD.csv\"\n",
    "Y_train = pd.DataFrame(Y_train)\n",
    "Y_train.to_csv(Y_train_prob_path, index=False)\n",
    "Y_test_prob_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\Y_test_NSL-KDD.csv\"\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "Y_test.to_csv(Y_test_prob_path, index=False)\n",
    "\n",
    "regressor = RandomForestRegressor(random_state=42)\n",
    "multioutput_regressor_RF = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "multioutput_regressor_RF.fit(X_train, Y_train)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"RF Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "#ADATRAIN\n",
    "regressor = AdaBoostRegressor(random_state=42)\n",
    "multioutput_regressor_ADA = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "multioutput_regressor_ADA.fit(X_train, Y_train)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"AdaBoostRegressor Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "#SVMTRAIN\n",
    "X_train_scale =preprocessing.scale(X_train)\n",
    "X_test_scale=preprocessing.scale(X_test)\n",
    "regressor = SVR(kernel='rbf')\n",
    "multioutput_regressor_SVM = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "multioutput_regressor_SVM.fit(X_train_scale, Y_train)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"SVR Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "#LGBMTRAIN\n",
    "regressor = LGBMRegressor(random_state=42)\n",
    "multioutput_regressor_LGBM = MultiOutputRegressor(regressor)\n",
    "start_train_time = time.time()\n",
    "multioutput_regressor_LGBM.fit(X_train, Y_train)\n",
    "end_train_time = time.time()\n",
    "training_time = end_train_time - start_train_time\n",
    "print(\"LGBMRegressor Model Trained: \\n\")\n",
    "print(\"\\nTime it took to train model: \", training_time)\n",
    "\n",
    "###################################################################################\n",
    "#############################RF TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_RF.predict(X_test)\n",
    "#TODO:// normalization of y_pred values from 0-1.  \n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"RF Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_RF.csv\"\n",
    "pred_scaled_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\scaled_probability_output_RF.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns = prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "#Convert to dataframe\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns=prob_output_column_names)\n",
    "\n",
    "df_y_pred.to_csv(pred_scaled_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_RF.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "\n",
    "ones_count_dict = {}\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in encoded_output.columns:\n",
    "    # Count the occurrences of 1 in the column\n",
    "    ones_count = (encoded_output[column] == 1).sum()\n",
    "    # Store the count of 1's in the dictionary\n",
    "    ones_count_dict[column] = ones_count\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "ones_count_df = pd.DataFrame.from_dict(ones_count_dict, orient='index',columns=['Frequency_count'])\n",
    "# Sort the DataFrame by the 'ones_count' column in descending order\n",
    "sorted_df = ones_count_df.sort_values(by='Frequency_count', ascending=False)\n",
    "\n",
    "# Select the top 5 rows\n",
    "top_5 = sorted_df.head(10)\n",
    "\n",
    "# Save the top 5 rows to a new CSV file\n",
    "top_5.to_csv('APS_top_5_frequency_count_RF.csv')\n",
    "\n",
    "# Divide each value in the 'Frequency count' column by 4510\n",
    "sorted_df['Accuracy_per_sample'] = sorted_df['Frequency_count'] / 4510\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "sorted_df.to_csv('Accuracy_per_sample_RF.csv', index=False)\n",
    "\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_1_RF.csv\"\n",
    "topk_1_RF = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_RF])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_5_RF.csv\"\n",
    "topk_5_RF = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_RF])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_10_RF.csv\"\n",
    "topk_10_RF = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_RF])\n",
    "df_topk_10.to_csv(topk_10, index=False)\n",
    "####################################################################################\n",
    "#############################ADA TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_ADA.predict(X_test)\n",
    "#TODO:// normalization of y_pred values from 0-1. \n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"ADABoostRegressor Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_ADA.csv\"\n",
    "pred_scaled_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\scaled_probability_output_ADA.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns=prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "#Convert to dataframe\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns=prob_output_column_names)\n",
    "\n",
    "df_y_pred.to_csv(pred_scaled_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_ADA.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "\n",
    "ones_count_dict = {}\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in encoded_output.columns:\n",
    "    # Count the occurrences of 1 in the column\n",
    "    ones_count = (encoded_output[column] == 1).sum()\n",
    "    # Store the count of 1's in the dictionary\n",
    "    ones_count_dict[column] = ones_count\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "ones_count_df = pd.DataFrame.from_dict(ones_count_dict, orient='index',columns=['Frequency_count'])\n",
    "# Sort the DataFrame by the 'ones_count' column in descending order\n",
    "sorted_df = ones_count_df.sort_values(by='Frequency_count', ascending=False)\n",
    "\n",
    "# Select the top 5 rows\n",
    "top_5 = sorted_df.head(10)\n",
    "\n",
    "# Save the top 5 rows to a new CSV file\n",
    "top_5.to_csv('APS_top_5_frequency_count_ADA.csv')\n",
    "\n",
    "# Divide each value in the 'Frequency count' column by 4510\n",
    "sorted_df['Accuracy_per_sample'] = sorted_df['Frequency_count'] / 4510\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "sorted_df.to_csv('Accuracy_per_sample_ADA.csv', index=False)\n",
    "\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_1_ADA.csv\"\n",
    "topk_1_ADA = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_ADA])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_5_ADA.csv\"\n",
    "topk_5_ADA = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_ADA])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_10_ADA.csv\"\n",
    "topk_10_ADA = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_ADA])\n",
    "df_topk_10.to_csv(topk_10, index=False)\n",
    "####################################################################################\n",
    "#############################SVM TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_SVM.predict(X_test_scale)\n",
    "#TODO:// normalization of y_pred values from 0-1. \n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"SVM Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_SVM.csv\"\n",
    "pred_scaled_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\scaled_probability_output_SVM.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns=prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "#Convert to dataframe\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns=prob_output_column_names)\n",
    "\n",
    "\n",
    "df_y_pred.to_csv(pred_scaled_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_SVM.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "\n",
    "ones_count_dict = {}\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in encoded_output.columns:\n",
    "    # Count the occurrences of 1 in the column\n",
    "    ones_count = (encoded_output[column] == 1).sum()\n",
    "    # Store the count of 1's in the dictionary\n",
    "    ones_count_dict[column] = ones_count\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "ones_count_df = pd.DataFrame.from_dict(ones_count_dict, orient='index',columns=['Frequency_count'])\n",
    "# Sort the DataFrame by the 'ones_count' column in descending order\n",
    "sorted_df = ones_count_df.sort_values(by='Frequency_count', ascending=False)\n",
    "\n",
    "# Select the top 5 rows\n",
    "top_5 = sorted_df.head(10)\n",
    "\n",
    "# Save the top 5 rows to a new CSV file\n",
    "top_5.to_csv('APS_top_5_frequency_count_SVM.csv')\n",
    "\n",
    "# Divide each value in the 'Frequency count' column by 4510\n",
    "sorted_df['Accuracy_per_sample'] = sorted_df['Frequency_count'] / 4510\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "sorted_df.to_csv('Accuracy_per_sample_SVM.csv', index=False)\n",
    "\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_1_SVM.csv\"\n",
    "topk_1_SVM = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_SVM])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_5_SVM.csv\"\n",
    "topk_5_SVM = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_SVM])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_10_SVM.csv\"\n",
    "topk_10_SVM = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_SVM])\n",
    "df_topk_10.to_csv(topk_10, index=False)\n",
    "\n",
    "####################################################################################\n",
    "#############################LGBM TESTING############################################\n",
    "start_test_time = time.time()\n",
    "y_pred = multioutput_regressor_LGBM.predict(X_test)\n",
    "#TODO:// normalization of y_pred values from 0-1. \n",
    "end_test_time = time.time()\n",
    "testing_time = end_test_time - start_test_time\n",
    "print(\"LGBM Model Tested: \\n\")\n",
    "print(\"\\nTime it took to test model: \", testing_time)\n",
    "\n",
    "Total_time = (training_time+testing_time)\n",
    "print(\"\\nTotal time taken for model training and testing: \", Total_time)\n",
    "\n",
    "Calculation_time = Total_time/(22544)\n",
    "print(\"\\nCalculation time for one sample: \", Calculation_time)\n",
    "\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "print(f'\\nMean Squared Error: {mse}\\n\\n\\n')\n",
    "\n",
    "pred_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_LGBM.csv\"\n",
    "pred_scaled_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\scaled_probability_output_LGBM.csv\"\n",
    "df_y_pred = pd.DataFrame(y_pred, columns=prob_output_column_names)\n",
    "df_y_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='uniform')\n",
    "df_y_pred = scaler.fit_transform(df_y_pred)\n",
    "#Convert to dataframe\n",
    "df_y_pred = pd.DataFrame(df_y_pred, columns=prob_output_column_names)\n",
    "\n",
    "df_y_pred.to_csv(pred_scaled_path, index=False)\n",
    "\n",
    "####################################Accuracy per Sample##############################\n",
    "output_path = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\probability_output_LGBM.csv\"\n",
    "output_dataset = pd.read_csv(output_path, header=0)\n",
    "threshold =.5\n",
    "encoded_output = output_dataset.apply(lambda x: (x > threshold).astype(int))\n",
    "\n",
    "ones_count_dict = {}\n",
    "# Iterate over each column in the DataFrame\n",
    "for column in encoded_output.columns:\n",
    "    # Count the occurrences of 1 in the column\n",
    "    ones_count = (encoded_output[column] == 1).sum()\n",
    "    # Store the count of 1's in the dictionary\n",
    "    ones_count_dict[column] = ones_count\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "ones_count_df = pd.DataFrame.from_dict(ones_count_dict, orient='index',columns=['Frequency_count'])\n",
    "# Sort the DataFrame by the 'ones_count' column in descending order\n",
    "sorted_df = ones_count_df.sort_values(by='Frequency_count', ascending=False)\n",
    "\n",
    "# Select the top 5 rows\n",
    "top_5 = sorted_df.head(10)\n",
    "\n",
    "# Save the top 5 rows to a new CSV file\n",
    "top_5.to_csv('APS_top_5_frequency_count_LGBM.csv')\n",
    "\n",
    "# Divide each value in the 'Frequency count' column by 4510\n",
    "sorted_df['Accuracy_per_sample'] = sorted_df['Frequency_count'] / 4510\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "sorted_df.to_csv('Accuracy_per_sample_LGBM.csv', index=False)\n",
    "\n",
    "##################################TOP MODEL PER SAMPLE##################################\n",
    "topk_1 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_1_LGBM.csv\"\n",
    "topk_1_LGBM = frequency_in_top_k(output_dataset, 1)\n",
    "df_topk_1 = pd.DataFrame([topk_1_LGBM])\n",
    "df_topk_1.to_csv(topk_1, index=False)\n",
    "\n",
    "topk_5 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_5_LGBM.csv\"\n",
    "topk_5_LGBM = frequency_in_top_k(output_dataset, 5)\n",
    "df_topk_5 = pd.DataFrame([topk_5_LGBM])\n",
    "df_topk_5.to_csv(topk_5, index=False)\n",
    "\n",
    "topk_10 = r\"C:\\Users\\kevin\\Desktop\\ECE RESEARCH\\ECE RESEARCH NSL-KDD\\topk_10_LGBM.csv\"\n",
    "topk_10_LGBM = frequency_in_top_k(output_dataset, 10)\n",
    "df_topk_10 = pd.DataFrame([topk_10_LGBM])\n",
    "df_topk_10.to_csv(topk_10, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
